{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7629d871-4cad-4e25-9f42-1f87234c8569",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aec528e-6842-4c63-83c9-ed73628ea567",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Config Tables Metadata Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5107c18-59e4-48b0-bd2a-1576a9b05dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "p_bds_db_name = dbutils.widgets.text(name=\"BDS_DB_NAME\", defaultValue=\"release_test\")\n",
    "BDS_DB_NAME = dbutils.widgets.get(name=\"BDS_DB_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2780562b-95be-4b3b-ab0e-71aa3cd9d73b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if BDS_DB_NAME in (\"\", None):\n",
    "    raise Exception(\"Invalid Database name. Halting the process for further steps...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b187806-489c-4291-a346-8beb54d39a81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_tables = {\n",
    "\n",
    "  \"test\":{\n",
    "        \"columns\":{\"test\":\"string\"}\n",
    "    },\n",
    "\n",
    "    \"bds_batch_metadata\": {  # key\n",
    "        \"columns\": {\n",
    "            \"md_batch_id\": \"string\",\n",
    "            \"md_batch_layer\": \"string\",\n",
    "            \"md_batch_start_time\": \"timestamp\",\n",
    "            \"md_batch_end_time\": \"timestamp\",\n",
    "            \"md_batch_details\": \"string\",\n",
    "            \"md_batch_status\": \"string\",\n",
    "            \"md_batch_failure_reason\": \"string\",\n",
    "            \"md_batch_failure_job_name\": \"struct\", #error string\n",
    "            \"md_batch_warning_msg\": \"string\",\n",
    "            \"md_batch_record_count_input\": \"int\",\n",
    "        }\n",
    "        },\n",
    "\n",
    "    \"bds_job_audit_log\": {\n",
    "        \"columns\": {\n",
    "            \"md_batch_id\": \"string\",\n",
    "            \"md_job_name\": \"string\",\n",
    "            \"md_batch_layer\": \"string\",\n",
    "            \"md_job_start_time\": \"timestamp\",\n",
    "            \"md_job_end_time\": \"timestamp\",\n",
    "            \"md_job_details\": \"string\",\n",
    "            \"md_job_status\": \"string\",\n",
    "            \"md_job_failure_reason\": \"string\",\n",
    "            \"md_job_record_count_input\": \"int\",\n",
    "            \"md_job_record_count_processed\": \"int\",\n",
    "            \"md_job_record_count_quarantined\": \"int\",\n",
    "            \"md_job_record_count_warning\": \"int\",\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"bds_quarantine_metadata\": {\n",
    "        \"columns\": {\n",
    "            \"transactionId\": \"string\",\n",
    "            \"json_message\": \"struct\",\n",
    "            \"batch_id\": \"int\",\n",
    "            \"created_timestamp\": \"timestamp\",\n",
    "            \"md_batch_id\": \"string\",\n",
    "            \"validation_errors\": \"string\",\n",
    "            \"md_batch_job_name\": \"string\",\n",
    "            \"md_batch_layer\": \"string\",\n",
    "            \"inserted_timestamp\": \"timestamp\",\n",
    "        }\n",
    "    },\n",
    "    \"bds_bronze_data\": {\n",
    "        \"columns\": {\n",
    "            \"batch_id\": \"int\",\n",
    "            \"transactionId\": \"string\",\n",
    "            \"com_kroger_desp_events_rss_rsssalestransaction\": \"struct\",\n",
    "            \"created_timestamp\": \"timestamp\",\n",
    "            \"md_batch_id\": \"string\",\n",
    "        }\n",
    "    },\n",
    "    \"test1\":{\n",
    "        \"columns\":{\"test\":\"string\"}\n",
    "    },\n",
    "    \"bds_bronze_validated\": {\n",
    "        \"columns\": {\n",
    "            \"transactionId\": \"string\",\n",
    "            \"com_kroger_desp_events_rss_rsssalestransaction\": \"string\", #error struct\n",
    "            \"created_timestamp\": \"timestamp\",\n",
    "            \"md_batch_id\": \"struct\", #error int\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7914b4c7-02b9-4e30-b99e-d077725d2bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_tables_df = spark.sql(f\"SHOW TABLES IN {BDS_DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c65158-dbc9-483c-9f94-18d34129cede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for 'test' table existence\nChecking for 'bds_batch_metadata' table existence\nChecking for 'bds_job_audit_log' table existence\nChecking for 'bds_quarantine_metadata' table existence\nChecking for 'bds_bronze_data' table existence\nChecking for 'test1' table existence\nChecking for 'bds_bronze_validated' table existence\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2307311006834220>:12\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m     if is_table_exists_df.count() == 0:\n",
       "\u001B[1;32m     10\u001B[0m         missing_tables.append(t)\n",
       "\u001B[0;32m---> 12\u001B[0m # Display missing tables\n",
       "\u001B[1;32m     13\u001B[0m if missing_tables:\n",
       "\u001B[1;32m     14\u001B[0m     raise Exception(f\"Required Tables Missing: {', '.join(missing_tables)}\")\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: Required Tables Missing: test, test1"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2307311006834220>:12\u001B[0m\n\u001B[1;32m      9\u001B[0m     if is_table_exists_df.count() == 0:\n\u001B[1;32m     10\u001B[0m         missing_tables.append(t)\n\u001B[0;32m---> 12\u001B[0m # Display missing tables\n\u001B[1;32m     13\u001B[0m if missing_tables:\n\u001B[1;32m     14\u001B[0m     raise Exception(f\"Required Tables Missing: {', '.join(missing_tables)}\")\n\n\u001B[0;31mException\u001B[0m: Required Tables Missing: test, test1",
       "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Required Tables Missing: test, test1",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Check for table existence and collect missing tables\n",
    "missing_tables = []\n",
    "for t in batch_tables:\n",
    "    print(f\"Checking for '{t}' table existence\")\n",
    "    is_table_exists_df = metadata_tables_df.filter(\n",
    "        metadata_tables_df.tableName.isin([t])\n",
    "    )\n",
    "    if is_table_exists_df.count() == 0:\n",
    "        missing_tables.append(t)\n",
    "# Display missing tables\n",
    "if missing_tables:\n",
    "    raise Exception(f\"Required Tables Missing: {', '.join(missing_tables)}\")\n",
    "print(\"All required tables are present.\")\n",
    "\n",
    "# Step 2: Collect schema validation issues\n",
    "validation_issues = []\n",
    "for t in batch_tables:\n",
    "    print(f\"Validating schema for '{t}' table\")\n",
    "\n",
    "    table_df = spark.sql(f\"DESCRIBE {BDS_DB_NAME}.{t}\")\n",
    "\n",
    "    schema_list = (\n",
    "        table_df.select(\"col_name\", \"data_type\")\n",
    "        .rdd.map(lambda row: (row[\"col_name\"], row[\"data_type\"]))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    schema_df = spark.createDataFrame(schema_list, [\"col_name\", \"data_type\"])\n",
    "\n",
    "    schema_df_with_hardcoded_type = schema_df.withColumn(\n",
    "        \"data_type\",\n",
    "        F.when(F.col(\"data_type\").contains(\"struct\"), \"struct\").otherwise(F.col(\"data_type\")),\n",
    "    )\n",
    " \n",
    "    schema_dict = schema_df_with_hardcoded_type.rdd.map(\n",
    "        lambda row: (row[\"col_name\"], row[\"data_type\"])\n",
    "    ).collectAsMap()\n",
    " \n",
    "    batch_columns = batch_tables[t][\"columns\"]\n",
    " \n",
    "    # Check for missing columns and data type mismatches\n",
    "    for col1, d_type in batch_columns.items():\n",
    "        if col1 not in schema_dict:\n",
    "            validation_issues.append(f\"Required column {col1} is missing in {BDS_DB_NAME}.{t}.\")\n",
    "        elif schema_dict[col1] != d_type:\n",
    "            validation_issues.append(\n",
    "                f\"Data type mismatch for column {col1} in {BDS_DB_NAME}.{t}: Expected {d_type}, Found {schema_dict[col1]}\"\n",
    "            )\n",
    " \n",
    "    for col1 in schema_dict:\n",
    "        if col1 not in batch_columns:\n",
    "            validation_issues.append(f\"Unexpected column {col1} found in {BDS_DB_NAME}.{t}.\")\n",
    "\n",
    "# Report all collected validation issues or confirm success\n",
    "if validation_issues:\n",
    "    raise Exception(\"Structure validation issues found:\\n\" + \"\\n\".join(validation_issues))\n",
    "else:\n",
    "    print(\"All schema validations passed successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764fe3db-804e-4696-8802-acc086090c83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"PASS: All tables and columns are present and correctly typed, proceed with executing the data pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "metadata_tables_check",
   "widgets": {
    "BDS_DB_NAME": {
     "currentValue": "release_test",
     "nuid": "add9be93-70d3-40ff-afc3-5b53ffa760d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "release_test",
      "label": null,
      "name": "BDS_DB_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "release_test",
      "label": null,
      "name": "BDS_DB_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
